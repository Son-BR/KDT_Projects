{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "---\n",
    " - 패키지 설치\n",
    "   - NLTK: !pip install nltk\n",
    "   - KoNLPy: !pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "---\n",
    " - 문장/문서를 의미를 지닌 작은 단위로 나누는 것\n",
    " - 나누어진 단어를 토큰(Token)이라고 함\n",
    " - 종류\n",
    "   - 문장 토큰화\n",
    "   - 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1='''The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing. A free online book is available. (If you use the library for academic research, please cite the book.)\n",
    "\n",
    "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O’Reilly Media Inc.'''\n",
    "\n",
    "raw_text2='''Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.\n",
    "\n",
    "This particular tokenizer requires the Punkt sentence tokenization models to be installed. NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open',\n",
       " 'source',\n",
       " 'Python',\n",
       " 'library',\n",
       " 'for',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'A',\n",
       " 'free',\n",
       " 'online',\n",
       " 'book',\n",
       " 'is',\n",
       " 'available',\n",
       " '.',\n",
       " '(',\n",
       " 'If',\n",
       " 'you',\n",
       " 'use',\n",
       " 'the',\n",
       " 'library',\n",
       " 'for',\n",
       " 'academic',\n",
       " 'research',\n",
       " ',',\n",
       " 'please',\n",
       " 'cite',\n",
       " 'the',\n",
       " 'book',\n",
       " '.',\n",
       " ')',\n",
       " 'Steven',\n",
       " 'Bird',\n",
       " ',',\n",
       " 'Ewan',\n",
       " 'Klein',\n",
       " ',',\n",
       " 'and',\n",
       " 'Edward',\n",
       " 'Loper',\n",
       " '(',\n",
       " '2009',\n",
       " ')',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'with',\n",
       " 'Python',\n",
       " '.',\n",
       " 'O',\n",
       " '’',\n",
       " 'Reilly',\n",
       " 'Media',\n",
       " 'Inc',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_result=sent_tokenize(raw_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Tokenizers divide strings into lists of substrings.',\n",
       "  'For example, tokenizers can be used to find the words and punctuation in a string.',\n",
       "  'This particular tokenizer requires the Punkt sentence tokenization models to be installed.',\n",
       "  'NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.'],\n",
       " 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_result, len(sent_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 문장에 토큰 추출\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing. A free online book is available. (If you use the library for academic research, please cite the book.)\n",
      "\n",
      "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O’Reilly Media Inc.\n",
      "-------\n",
      "['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.', 'A', 'free', 'online', 'book', 'is', 'available', '.', '(', 'If', 'you', 'use', 'the', 'library', 'for', 'academic', 'research', ',', 'please', 'cite', 'the', 'book', '.', ')', 'Steven', 'Bird', ',', 'Ewan', 'Klein', ',', 'and', 'Edward', 'Loper', '(', '2009', ')', '.', 'Natural', 'Language', 'Processing', 'with', 'Python', '.', 'O', '’', 'Reilly', 'Media', 'Inc', '.']\n",
      "-------\n",
      "sent: Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.\n",
      "\n",
      "This particular tokenizer requires the Punkt sentence tokenization models to be installed. NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.\n",
      "-------\n",
      "['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.', 'For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string', '.', 'This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.', 'NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n",
      "-------\n",
      "[['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.', 'A', 'free', 'online', 'book', 'is', 'available', '.', '(', 'If', 'you', 'use', 'the', 'library', 'for', 'academic', 'research', ',', 'please', 'cite', 'the', 'book', '.', ')', 'Steven', 'Bird', ',', 'Ewan', 'Klein', ',', 'and', 'Edward', 'Loper', '(', '2009', ')', '.', 'Natural', 'Language', 'Processing', 'with', 'Python', '.', 'O', '’', 'Reilly', 'Media', 'Inc', '.'], ['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.', 'For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string', '.', 'This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.', 'NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 문장단위로 추출\n",
    "raw_text=[raw_text1, raw_text2]\n",
    "total_token=[]\n",
    "\n",
    "# 문장 추출\n",
    "for sent in raw_text:\n",
    "    print(f'sent: {sent}', '-------', sep='\\n')\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    sentToken=word_tokenize(sent)\n",
    "    print(sentToken, '-------', sep='\\n')\n",
    "    # 모든 문장의 토큰에 추가\n",
    "    total_token.append(sentToken)\n",
    "    \n",
    "print(total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: The Natural Language Toolkit (NLTK) is an open source Python library for Natural Language Processing. A free online book is available. (If you use the library for academic research, please cite the book.)\n",
      "\n",
      "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O’Reilly Media Inc.\n",
      "-------\n",
      "['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.']\n",
      "-------\n",
      "['A', 'free', 'online', 'book', 'is', 'available', '.']\n",
      "-------\n",
      "['(', 'If', 'you', 'use', 'the', 'library', 'for', 'academic', 'research', ',', 'please', 'cite', 'the', 'book', '.', ')']\n",
      "-------\n",
      "['Steven', 'Bird', ',', 'Ewan', 'Klein', ',', 'and', 'Edward', 'Loper', '(', '2009', ')', '.']\n",
      "-------\n",
      "['Natural', 'Language', 'Processing', 'with', 'Python', '.']\n",
      "-------\n",
      "['O', '’', 'Reilly', 'Media', 'Inc', '.']\n",
      "-------\n",
      "sent: Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.\n",
      "\n",
      "This particular tokenizer requires the Punkt sentence tokenization models to be installed. NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation.\n",
      "-------\n",
      "['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.']\n",
      "-------\n",
      "['For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string', '.']\n",
      "-------\n",
      "['This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.']\n",
      "-------\n",
      "['NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']\n",
      "-------\n",
      "[['The', 'Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'an', 'open', 'source', 'Python', 'library', 'for', 'Natural', 'Language', 'Processing', '.'], ['A', 'free', 'online', 'book', 'is', 'available', '.'], ['(', 'If', 'you', 'use', 'the', 'library', 'for', 'academic', 'research', ',', 'please', 'cite', 'the', 'book', '.', ')'], ['Steven', 'Bird', ',', 'Ewan', 'Klein', ',', 'and', 'Edward', 'Loper', '(', '2009', ')', '.'], ['Natural', 'Language', 'Processing', 'with', 'Python', '.'], ['O', '’', 'Reilly', 'Media', 'Inc', '.'], ['Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.'], ['For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string', '.'], ['This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.'], ['NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 문장단위로 추출\n",
    "raw_text=[raw_text1, raw_text2]\n",
    "total_token=[]\n",
    "\n",
    "# 문장 추출\n",
    "for sent in raw_text:\n",
    "    print(f'sent: {sent}', '-------', sep='\\n')\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    for ele in sentResult:\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(wordResult, '-------', sep='\\n')\n",
    "        total_token.append(wordResult)\n",
    "    \n",
    "print(total_token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '은', '월요일', '입니다', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분리\n",
    "result=okt.morphs('오늘은 월요일입니다.')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('월요일', 'Noun'),\n",
       " ('입니다', 'Adjective'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분리 후 태깅(Tagging) -> 품사\n",
    "result2=okt.pos('오늘은 월요일입니다.')\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('월요일', 'Noun'),\n",
       " ('이다', 'Adjective'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2=okt.pos('오늘은 월요일입니다.', stem=True)\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "---\n",
    " - 불용어 제거 -> 노이즈 제거\n",
    " - 텍스트의 동일화\n",
    "   - 대문자 또는 소문자로 통일\n",
    "   - 문장의 길이\n",
    "\n",
    "### [2-1] 불용어(Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179,\n",
       " ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\"])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')\n",
    "len(en_stopwords), en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-2] 어간 및 표제어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work', 'work')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('working'),lstem.stem('works'),lstem.stem('worked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('working', 'v'), wlemma.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlemma.lemmatize('amusing', 'v'), wlemma.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] 텍스트 벡터화\n",
    "---\n",
    " - 텍스트 -> 수치화\n",
    " - 희소벡터(OHE): BOW방식 -> Count기반, TF-IDF 기반\n",
    " - 밀집벡터: Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1, raw_text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(corpus)\n",
    "ret=ohe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t2\n",
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 18)\t2\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 25)\t2\n",
      "  (0, 26)\t1\n",
      "  (0, 27)\t3\n",
      "  (0, 28)\t2\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 33)\t3\n",
      "  (0, 34)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 40)\t1\n",
      "  :\t:\n",
      "  (1, 35)\t1\n",
      "  (1, 36)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t2\n",
      "  (1, 44)\t1\n",
      "  (1, 46)\t1\n",
      "  (1, 48)\t1\n",
      "  (1, 50)\t1\n",
      "  (1, 51)\t1\n",
      "  (1, 53)\t1\n",
      "  (1, 55)\t1\n",
      "  (1, 56)\t1\n",
      "  (1, 57)\t1\n",
      "  (1, 58)\t1\n",
      "  (1, 59)\t2\n",
      "  (1, 60)\t1\n",
      "  (1, 61)\t2\n",
      "  (1, 62)\t1\n",
      "  (1, 63)\t2\n",
      "  (1, 64)\t2\n",
      "  (1, 67)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 69)\t1\n",
      "  (1, 71)\t1\n"
     ]
    }
   ],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 73) [[1 1 0 1 1 1 0 0 1 2 0 1 0 1 1 0 0 0 2 1 1 0 1 0 0 2 1 3 2 0 1 1 0 3 1 0\n",
      "  0 1 1 0 1 2 0 0 0 2 0 1 0 1 0 0 1 0 1 0 0 0 0 3 0 0 0 0 0 1 1 0 0 0 1 0\n",
      "  1]\n",
      " [0 0 1 0 2 0 1 2 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
      "  1 0 0 1 0 0 1 2 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 2 1 2 1 2 2 0 0 1 1 1 0 1\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF 기반\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11901923 0.11901923 0.         0.11901923 0.08468317 0.11901923\n",
      "  0.         0.         0.11901923 0.23803847 0.         0.11901923\n",
      "  0.         0.11901923 0.11901923 0.         0.         0.\n",
      "  0.16936633 0.11901923 0.11901923 0.         0.11901923 0.\n",
      "  0.         0.23803847 0.11901923 0.3570577  0.23803847 0.\n",
      "  0.11901923 0.11901923 0.         0.3570577  0.08468317 0.\n",
      "  0.         0.11901923 0.11901923 0.         0.11901923 0.23803847\n",
      "  0.         0.         0.         0.23803847 0.         0.11901923\n",
      "  0.         0.11901923 0.         0.         0.11901923 0.\n",
      "  0.11901923 0.         0.         0.         0.         0.2540495\n",
      "  0.         0.         0.         0.         0.         0.11901923\n",
      "  0.11901923 0.         0.         0.         0.11901923 0.\n",
      "  0.11901923]\n",
      " [0.         0.         0.13238075 0.         0.18837999 0.\n",
      "  0.13238075 0.2647615  0.         0.         0.13238075 0.\n",
      "  0.13238075 0.         0.         0.13238075 0.13238075 0.13238075\n",
      "  0.09418999 0.         0.         0.13238075 0.         0.13238075\n",
      "  0.13238075 0.         0.         0.         0.         0.13238075\n",
      "  0.         0.         0.13238075 0.         0.09418999 0.13238075\n",
      "  0.13238075 0.         0.         0.13238075 0.         0.\n",
      "  0.13238075 0.2647615  0.13238075 0.         0.13238075 0.\n",
      "  0.13238075 0.         0.13238075 0.13238075 0.         0.13238075\n",
      "  0.         0.13238075 0.13238075 0.13238075 0.13238075 0.18837999\n",
      "  0.13238075 0.2647615  0.13238075 0.2647615  0.2647615  0.\n",
      "  0.         0.13238075 0.13238075 0.13238075 0.         0.13238075\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화, 불용어 제거\n",
    "\n",
    "sent='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiki',\n",
       " 'is',\n",
       " 'in',\n",
       " 'Ward',\n",
       " 'is',\n",
       " 'original',\n",
       " 'description',\n",
       " ':',\n",
       " 'The',\n",
       " 'simplest',\n",
       " 'online',\n",
       " 'database',\n",
       " 'that',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'work.Wiki',\n",
       " 'is',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'server',\n",
       " 'software',\n",
       " 'that',\n",
       " 'allows',\n",
       " 'users',\n",
       " 'to',\n",
       " 'freely',\n",
       " 'create',\n",
       " 'and',\n",
       " 'edit',\n",
       " 'Web',\n",
       " 'page',\n",
       " 'content',\n",
       " 'using',\n",
       " 'any',\n",
       " 'Web',\n",
       " 'browser',\n",
       " '.',\n",
       " 'Wiki',\n",
       " 'supports',\n",
       " 'hyperlinks',\n",
       " 'and',\n",
       " 'has',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'text',\n",
       " 'syntax',\n",
       " 'for',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'pages',\n",
       " 'and',\n",
       " 'crosslinks',\n",
       " 'between',\n",
       " 'internal',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fly.Wiki',\n",
       " 'is',\n",
       " 'unusual',\n",
       " 'among',\n",
       " 'group',\n",
       " 'communication',\n",
       " 'mechanisms',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " 'allows',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'of',\n",
       " 'contributions',\n",
       " 'to',\n",
       " 'be',\n",
       " 'edited',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'content',\n",
       " 'itself.Like',\n",
       " 'many',\n",
       " 'simple',\n",
       " 'concepts',\n",
       " ',',\n",
       " '``',\n",
       " 'open',\n",
       " 'editing',\n",
       " \"''\",\n",
       " 'has',\n",
       " 'some',\n",
       " 'profound',\n",
       " 'and',\n",
       " 'subtle',\n",
       " 'effects',\n",
       " 'on',\n",
       " 'Wiki',\n",
       " 'usage',\n",
       " '.',\n",
       " 'Allowing',\n",
       " 'everyday',\n",
       " 'users',\n",
       " 'to',\n",
       " 'create',\n",
       " 'and',\n",
       " 'edit',\n",
       " 'any',\n",
       " 'page',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Web',\n",
       " 'site',\n",
       " 'is',\n",
       " 'exciting',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " 'encourages',\n",
       " 'democratic',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Web',\n",
       " 'and',\n",
       " 'promotes',\n",
       " 'content',\n",
       " 'composition',\n",
       " 'by',\n",
       " 'nontechnical',\n",
       " 'users',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_result=word_tokenize(sent)\n",
    "\n",
    "word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "\n",
    "for w in word_result:\n",
    "    if w not in en_stopwords:\n",
    "        result.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2=[word for word in word_result if word not in en_stopwords]\n",
    "len(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki',\n",
       " 'is',\n",
       " 'in',\n",
       " 'ward',\n",
       " 'is',\n",
       " 'original',\n",
       " 'description',\n",
       " 'the',\n",
       " 'simplest',\n",
       " 'online',\n",
       " 'database',\n",
       " 'that',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'work',\n",
       " 'wiki',\n",
       " 'is',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'server',\n",
       " 'software',\n",
       " 'that',\n",
       " 'allows',\n",
       " 'users',\n",
       " 'to',\n",
       " 'freely',\n",
       " 'create',\n",
       " 'and',\n",
       " 'edit',\n",
       " 'web',\n",
       " 'page',\n",
       " 'content',\n",
       " 'using',\n",
       " 'any',\n",
       " 'web',\n",
       " 'browser',\n",
       " 'wiki',\n",
       " 'supports',\n",
       " 'hyperlinks',\n",
       " 'and',\n",
       " 'has',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'text',\n",
       " 'syntax',\n",
       " 'for',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'pages',\n",
       " 'and',\n",
       " 'crosslinks',\n",
       " 'between',\n",
       " 'internal',\n",
       " 'pages',\n",
       " 'on',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'wiki',\n",
       " 'is',\n",
       " 'unusual',\n",
       " 'among',\n",
       " 'group',\n",
       " 'communication',\n",
       " 'mechanisms',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " 'allows',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'of',\n",
       " 'contributions',\n",
       " 'to',\n",
       " 'be',\n",
       " 'edited',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'content',\n",
       " 'itself',\n",
       " 'like',\n",
       " 'many',\n",
       " 'simple',\n",
       " 'concepts',\n",
       " 'open',\n",
       " 'editing',\n",
       " 'has',\n",
       " 'some',\n",
       " 'profound',\n",
       " 'and',\n",
       " 'subtle',\n",
       " 'effects',\n",
       " 'on',\n",
       " 'wiki',\n",
       " 'usage',\n",
       " 'allowing',\n",
       " 'everyday',\n",
       " 'users',\n",
       " 'to',\n",
       " 'create',\n",
       " 'and',\n",
       " 'edit',\n",
       " 'any',\n",
       " 'page',\n",
       " 'in',\n",
       " 'a',\n",
       " 'web',\n",
       " 'site',\n",
       " 'is',\n",
       " 'exciting',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " 'encourages',\n",
       " 'democratic',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'web',\n",
       " 'and',\n",
       " 'promotes',\n",
       " 'content',\n",
       " 'composition',\n",
       " 'by',\n",
       " 'nontechnical',\n",
       " 'users']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'wiki': 2, 'is': 3, 'in': 4, 'the': 5, 'that': 6, 'to': 7, 'web': 8, 'a': 9, 'of': 10, 'users': 11, 'content': 12, 'allows': 13, 'create': 14, 'edit': 15, 'page': 16, 'any': 17, 'has': 18, 'simple': 19, 'pages': 20, 'on': 21, 'it': 22, 'ward': 23, 'original': 24, 'description': 25, 'simplest': 26, 'online': 27, 'database': 28, 'could': 29, 'possibly': 30, 'work': 31, 'piece': 32, 'server': 33, 'software': 34, 'freely': 35, 'using': 36, 'browser': 37, 'supports': 38, 'hyperlinks': 39, 'text': 40, 'syntax': 41, 'for': 42, 'creating': 43, 'new': 44, 'crosslinks': 45, 'between': 46, 'internal': 47, 'fly': 48, 'unusual': 49, 'among': 50, 'group': 51, 'communication': 52, 'mechanisms': 53, 'organization': 54, 'contributions': 55, 'be': 56, 'edited': 57, 'addition': 58, 'itself': 59, 'like': 60, 'many': 61, 'concepts': 62, 'open': 63, 'editing': 64, 'some': 65, 'profound': 66, 'subtle': 67, 'effects': 68, 'usage': 69, 'allowing': 70, 'everyday': 71, 'site': 72, 'exciting': 73, 'encourages': 74, 'democratic': 75, 'use': 76, 'promotes': 77, 'composition': 78, 'by': 79, 'nontechnical': 80}\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('wiki', 5), ('is', 5), ('in', 5), ('ward', 1), ('original', 1), ('description', 1), ('the', 5), ('simplest', 1), ('online', 1), ('database', 1), ('that', 4), ('could', 1), ('possibly', 1), ('work', 1), ('a', 3), ('piece', 1), ('of', 3), ('server', 1), ('software', 1), ('allows', 2), ('users', 3), ('to', 4), ('freely', 1), ('create', 2), ('and', 6), ('edit', 2), ('web', 4), ('page', 2), ('content', 3), ('using', 1), ('any', 2), ('browser', 1), ('supports', 1), ('hyperlinks', 1), ('has', 2), ('simple', 2), ('text', 1), ('syntax', 1), ('for', 1), ('creating', 1), ('new', 1), ('pages', 2), ('crosslinks', 1), ('between', 1), ('internal', 1), ('on', 2), ('fly', 1), ('unusual', 1), ('among', 1), ('group', 1), ('communication', 1), ('mechanisms', 1), ('it', 2), ('organization', 1), ('contributions', 1), ('be', 1), ('edited', 1), ('addition', 1), ('itself', 1), ('like', 1), ('many', 1), ('concepts', 1), ('open', 1), ('editing', 1), ('some', 1), ('profound', 1), ('subtle', 1), ('effects', 1), ('usage', 1), ('allowing', 1), ('everyday', 1), ('site', 1), ('exciting', 1), ('encourages', 1), ('democratic', 1), ('use', 1), ('promotes', 1), ('composition', 1), ('by', 1), ('nontechnical', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [3]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myToken.texts_to_sequences(['and','is'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " - 제공한 문서/문장에 대한 단어사전(vaca)\n",
    " - 단어사전(voca)에 존재하지 않는 단어 -> Out Of Voca: OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "                    # num_words=:.texts_to_sequences에 표시할 빈도높은 단어갯수\n",
    "                    # oov_token=: oov의 인덱스를 설정\n",
    "\n",
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'love': 2,\n",
       " 'dog': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'cat': 6,\n",
       " 'do': 7,\n",
       " 'think': 8,\n",
       " 'is': 9,\n",
       " 'amazing': 10}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('i', 2),\n",
       "             ('love', 3),\n",
       "             ('my', 4),\n",
       "             ('dog', 3),\n",
       "             ('cat', 1),\n",
       "             ('you', 2),\n",
       "             ('do', 1),\n",
       "             ('think', 1),\n",
       "             ('is', 1),\n",
       "             ('amazing', 1)])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 출력 갯수\n",
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 생성된 사전(vaca)을 기반으로 수치화\n",
    "seq_voca=tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Encording 변환\n",
    "---\n",
    " - sklearn OneHotEncoder 객체생성\n",
    " - keras 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_voca), seq_voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(seq_voca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩(Padding)\n",
    "---\n",
    " - 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    " - 길이 기준 설정\n",
    " - 긴 경우 -> 앞/뒤 중 선택(값 제거 위치)\n",
    " - 짧은 경우 -> 앞/뒤 중 선택(값 채울 위치)\n",
    " - 값 -> 패딩에 들어갈 값\n",
    " - OHE 전에 하는듯 아마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=tokenizer.texts_to_sequences(sentences)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  4,  2,  1,  3],\n",
       "       [ 0,  0,  0,  4,  2,  1,  6],\n",
       "       [ 0,  0,  0,  5,  2,  1,  3],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding=pad_sequences(result)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
